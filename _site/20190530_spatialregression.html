<!DOCTYPE HTML>
<html>
<head>
  <title>MistsOfData</title>
  <meta charset="utf-8">
  <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400italic,700|Open+Sans+Condensed:300,700" rel="stylesheet">
  <script src="js/jquery-1.8.3.min.js"></script>
  <script src="css/5grid/init.js?use=mobile,desktop,1200px,1000px&amp;mobileUI=1&amp;mobileUI.theme=none"></script>
  <noscript>
  <link rel="stylesheet" href="css/5grid/core.css">
  <link rel="stylesheet" href="css/5grid/core-desktop.css">
  <link rel="stylesheet" href="css/5grid/core-1200px.css">
  <link rel="stylesheet" href="css/5grid/core-noscript.css">
  <link rel="stylesheet" href="css/style.css">
  <link rel="stylesheet" href="css/style-desktop.css">
  <link rel="stylesheet" href="css/style-1200px.css">
  </noscript>
</head>

<body class="left-sidebar">
  <div id="wrapper">
    <link href="css/prism/prism.css" rel="stylesheet" type="text/css" />
    <script src="css/prism/prism.js" type="text/javascript"></script>
    <div id="sidebar">
  <div id="logo"> <h1 class="mobileUI-site-name">Mists of Data</h1> </div>
  <nav id="nav" class="mobileUI-site-nav">
    <ul>
      <li class="current_page_item"><a href="mistsofdata.html">Latest Post</a></li>
      <li><a href="blog_archive.html">Archive</a></li>
      <li><a href="index.html">About Me</a></li>
      <!-- <li><a href="#">Contact Me</a></li> -->
    </ul>
  </nav>

  <!--
  <section class="is-search is-first">
    <form method="post" action="#">
      <input type="text" class="text" name="search" placeholder="Search">
    </form>
  </section>

  <section class="is-text-style1">
    <div class="inner">
      <p> <strong>Striped:</strong> A free and fully responsive HTML5 site template designed by AJ for HTML5 Up! </p>
    </div>
  </section>
  -->

  <!--
  <section class="is-recent-posts">
    <header>
      <h2>Recent Posts</h2>
    </header>
    <ul>
      <li><a href="#">Nothing happened</a></li>
      <li><a href="#">My Dearest Cthulhu</a></li>
      <li><a href="#">The Meme Meme</a></li>
      <li><a href="#">Now Full Cyborg</a></li>
      <li><a href="#">Temporal Flux</a></li>
    </ul>
  </section>
  <section class="is-recent-comments">
    <header>
      <h2>Recent Comments</h2>
    </header>
    <ul>
      <li>case on <a href="#">Now Full Cyborg</a></li>
      <li>molly on <a href="#">Untitled Post</a></li>
      <li>case on <a href="#">Temporal Flux</a></li>
    </ul>
  -->

  </section>
  <!--
  <section class="is-calendar">
    <div class="inner">
      <table>
        <caption>
        February 2045
        </caption>
        <thead>
          <tr>
            <th scope="col" title="Monday">M</th>
            <th scope="col" title="Tuesday">T</th>
            <th scope="col" title="Wednesday">W</th>
            <th scope="col" title="Thursday">T</th>
            <th scope="col" title="Friday">F</th>
            <th scope="col" title="Saturday">S</th>
            <th scope="col" title="Sunday">S</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="4" class="pad"><span>&nbsp;</span></td>
            <td><span>1</span></td>
            <td><span>2</span></td>
            <td><span>3</span></td>
          </tr>
          <tr>
            <td><span>4</span></td>
            <td><span>5</span></td>
            <td><a href="#">6</a></td>
            <td><span>7</span></td>
            <td><span>8</span></td>
            <td><span>9</span></td>
            <td><a href="#">10</a></td>
          </tr>
          <tr>
            <td><span>11</span></td>
            <td><span>12</span></td>
            <td><span>13</span></td>
            <td class="today"><a href="#">14</a></td>
            <td><span>15</span></td>
            <td><span>16</span></td>
            <td><span>17</span></td>
          </tr>
          <tr>
            <td><span>18</span></td>
            <td><span>19</span></td>
            <td><span>20</span></td>
            <td><span>21</span></td>
            <td><span>22</span></td>
            <td><a href="#">23</a></td>
            <td><span>24</span></td>
          </tr>
          <tr>
            <td><a href="#">25</a></td>
            <td><span>26</span></td>
            <td><span>27</span></td>
            <td><span>28</span></td>
            <td class="pad" colspan="3"><span>&nbsp;</span></td>
          </tr>
        </tbody>
      </table>
    </div>
  -->
  </section>
  <pre>












  </pre>
  <div id="copyright">
    <p> &copy; 2018 MistsOfData<br>
      Images: <a href="http://n33.co">n33</a>, <a href="http://fotogrph.com">Fotogrph</a>, <a href="http://iconify.it">Iconify.it</a>
      Design: <a href="http://html5up.net/">HTML5 Up!</a> </p>
  </div>
</div>

       <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
</script>

<div id="content" class="mobileUI-main-content">
  <div id="content-inner">
    <article class="is-post is-post-excerpt">

      <header>
        <h2>Spatial Regression</h2>
        <span class="byline">An introduction using R and RStan</span>
      </header>

      <div class="info">
        <span class="date">
          <span class="month">May</span><span class="day">30</span><span class="year">2019</span>
        </span>
        <ul class="stats">
          <!--
            <li><a href="#" class="link-icon24 link-icon24-1">16</a></li>
            <li><a href="#" class="link-icon24 link-icon24-2">32</a></li>
            <li><a href="#" class="link-icon24 link-icon24-3">64</a></li>
            <li><a href="#" class="link-icon24 link-icon24-4">128</a></li>
          -->
        </ul>
      </div>

      <!--
      <a href="#" class="image image-full"><img src="images/n33-robot-invader.jpg" alt=""></a>
      -->

      <p><b>About this post</b>: During the past couple of years I have been giving an introduction
      seminar to Spatial Regression using R. I have decided to put part of the seminar's material here.</p>

      <a name="outline"></a>
      <span class="byline">Outline</span>

      <ul>
        <li> 1. Introduction</li>
        <li> 2. About the code used</li>
        <li> 3. Linear Regression</li>
        <li> 4. Trend Surface</li>
        <li> 5. Covariance Functions</li>
        <li> 6. Geostatistical Process</li>
        <li> 7. References</li>
      </ul>



      <a name="introduction"></a>
      <span class="byline">1. Introduction</span>
      <p><i>"Everything is related to everything, but near things are more
      related than distant things"</i>
      <a href="https://en.wikipedia.org/wiki/Waldo_R._Tobler">[Waldo Tobler]</a>

      <p>This law of Geography can be translated to the language of Statistics
      as: things nearby are more alike and thus we can expect them to be
      positively correlated. The core idea behind Spatial Statistics is to
      understand an characterize this spatial dependence that is observed in
      different processes, for example: amount of rainfall, global temperature,
      air pollution, etc.</p>

      <p>Spatial dependence, as a concept, started to be formally addressed by
      statisticians at the beginning of the XX century (Fisher, 1935). However,
      it was until the second half of the century when Matheron (1962, 1963)
      and Gandin (1963) developed a
      <a href="https://en.wikipedia.org/wiki/Best_linear_unbiased_prediction">best
        linear unbiased predictor (blup)</a> for spatial modeling. Kriging, as
      the predictor is known, became the standard modeling approach of
      continuous spatial variation.</p>

      <p>Geostatistical methods have traditionally been related to data observed at a
      set of spatial locations indexed in a continuous space. This is as opposed to
      lattice methods and point pattern methods. The former related to data observed
      on a fixed set locations and the latter used for data associated to a point
      process. Diggle et al. (2013) consider that distinctions based on data formats
      are not always appropriate. They argue that the main theoretical distinction
      within spatial statistics is the continuity or non-continuity of the process
      being modeled. The model-based approach (Diggle et al., 1998) has become the
      current paradigm for modelling variability and quantifying uncertainty: a
      hierarchical thinking that explicitly assumes a stochastic model, but also
      acknowledges a different uncertainty in the data and in the parameters of the
      process (Cressie and Wikle, 2011).</p>

      <a name="code"></a>
      <span class="byline">2. About the Code Used</span>

      <p>Along the discussion we will show some interactive examples. We will
      use some auxiliary code for running these examples, which will be
      available for everybody to download from
      <a href="https://github.com/ric70x7/Playground/tree/master/SpatialStats">
        this repository</a>. The following R packages need to be
      preinstalled for the code of this session to work:</p>

<pre><code class="language-r">MASS
pracma
GPFDA
RandomFields
caret
rstan
Rcpp
ggplot2
viridis
</code></pre>

      <p>In general, we will call the methods of the packages as:</p>

<pre><code class="language-r">package_name::method_name()</code></pre>

      <p>This is as opposed to loading the package and calling the method directly:</p>

<pre><code class="language-r">library(package_name)
method_name()
</code></pre>

      <p>We will do this so that it is explicit which method belongs to which
      package. The exceptions to this rule will be <i>ggplo2</i> and <i>Rccp</i>. The
      former because otherwise notation becomes too long and less readable when
      making plots, and the latter because otherwise we will run into problems when
      calling <i>rstan</i>.

      <p>The command below will load ggplot, Rcpp and the auxiliary code
      needed for the session.</p>

<pre><code class="language-r">library(ggplot2)
library(Rcpp)
source("r_code/auxiliary_functions.R")
</code></pre>

      <a name="linear_regression"></a>
      <span class="byline">3. Linear Regression</span>

      <p>As a first step it will be convenient to have a brief recap on linear
      regression. Let \( \{ y_i, x_{i1}, \ldots, x_{ip} \}_{i=1}^n \) be a set of
      dependent and explanatory variables. In Ordinary Least Squares we usually
      want to estimate the parameters \( \{\beta_j\}_{j=0}^{p} \) that best fit the
      model</p>

      <p>\( y_i = \beta_0 + \beta_1  x_{i1} + \ldots + \beta_j x_{ip} +
      \epsilon_i\) for \(i = 1, \ldots, n.\)</p>

      <p>where \( \epsilon_i \) a Gaussian random variable with mean zero. The
      parameters chosen are the ones that generate the hyperplane that is
      closest on average to all the points \( x_{ij} \).</p>

<pre><code class="language-r">## Example

# Generate some toy data
reg_data <- lm_data(seed=0)
reg_data$type <- 'observed'

# Fit linear model
model1 <- lm(y ~ x, data=reg_data)

# Predictions
x_linspace <- seq(.5, 2.5 * pi, length.out=100) 
to_append <- data.frame(x=x_linspace, y=predict(model1, newdata=data.frame(x=x_linspace)), type='order_1')
reg_data <- rbind(reg_data, to_append)

# Plot results
ggplot(data=subset(reg_data, type=='observed'), aes(x, y)) +
       geom_point(col="steelblue", size=2) + 
       geom_line(data=subset(reg_data, type=='order_1'), col="red") +
       ggtitle(label = "Linear Regression Example") + 
       theme_minimal()
</code></pre>

<img src="images/20190530_spatialregression/linear_reg.png" alt="linear_regression_example_1">

      <p>Sometimes (almost every time) a straight line is not enough to explain the
      relationship between \(x_i\) and \(y_i\). This usually makes us start thinking of
      some polynomial regression that is more adequate. For example:</p>

      <p>Order 2: \( y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \epsilon_i \)</p>

      <p>Order 3: \( y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i \) </p>

<pre><code class="language-r">n_poly <- 4
poly_data <- reg_data
for (i in 2:n_poly) {
    new_dataset <- subset(reg_data, type=='observed')[,-c(3)]
    new_linspace <- data.frame(x=x_linspace)
    for (j in 2:i) {
        new_dataset[paste0("x", j)] <- new_dataset$x^j
        new_linspace[paste0("x", j)] <- new_linspace$x^j
    }
    m <- lm(y ~ ., data = new_dataset)
    to_append <- data.frame(x=x_linspace,
                            y=predict(m, newdata=new_linspace),
                            type=paste("order", i, sep="_"))
    poly_data <- rbind(poly_data, to_append)
}

# Plot results
ggplot(data=subset(poly_data, type=='observed'), aes(x, y)) +
       geom_point(col="steelblue", size=2) +
       geom_line(data=subset(poly_data, type!='observed'), aes(color=type)) +
       ggtitle(label = "Polynomial Regression Example") +
       theme_minimal()
</code></pre>

<img src="images/20190530_spatialregression/polynomial_reg.png" alt="linear_regression_example_2">

      <a name="trend_surface"></a>
      <span class="byline">4. Trend Surface</span>

      <p>Linear regression models can be used to summarize the variation in
      spatial patterns. A trend surface of order \(n\) can be estimated by
      applying a regression model using the spatial coordinates as the
      independent variables. For example, let the longitude and latitude
      coordinates be defined as \(x_{i1}\) and \(x_{i2}\) respectively, then we can
      describe the following trend surfaces:</p>

      <p>Order 1: \( y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i1}
      x_{i2} + \epsilon_i\)</p>

      <p>Order 2: \( y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i1}
      x_{i2} + \beta_4 x_{i1}^2 + \beta_5 x_{i2}^2 + \epsilon_i \)</p>

      <p>Note that we are also adding the product \(x_{i1} x_{i2}\). This is a term
      that accounts for the interactions.</p>

      <p>Having a trend across the surface seems like a good idea. However we
      will see that it is not the best approach.</p>

      <p>First Now, we will make a new toy dataset to see how this approach
      works.</p>

      <a name="soil_survey"></a>
      <span class="byline">Example: Soil Survey</span>

      <p>Imagine that we have a plot of \(400m \times 400m\) where we want to
      study a specific property of the soil, we will call it <i>soiliness</i>. We
      pick different locations at random and take measurements of soiliness.</p>

<pre><code class="language-r"> ## Soil example
spatial_reg <- soil_data(n_peaks=2, n_data = 150, seed=0)
head(spatial_reg)

ggplot(spatial_reg, aes(lng, lat)) +
        geom_point(aes(col=soiliness), size=2.5) +
        viridis::scale_color_viridis(option="plasma") +
        ggtitle("Clustered Spatial Data") +
        theme_minimal()
</code></pre>

<img src="images/20190530_spatialregression/soil_data.png" alt="soil_data">

      <p>Below we show the resulting fitted surfaces.</p>

<pre><code class="language-r"># Fit a model with a trend of order 1
surf_order1 <- lm(soiliness ~ lng * lat, data=spatial_reg)

# Fit a model with a trend of order 2
surf_order2 <- lm(soiliness ~ lng * lat + I(lng^2) + I(lat^2), data=spatial_reg)

# Grid for predictions
surf_grid <- as.data.frame(make_grid(size = 20))

surf_grid$surface_o1 <- predict(surf_order1, newdata=surf_grid)
surf_grid$surface_o2 <- predict(surf_order2, newdata=surf_grid)

ggplot(surf_grid, aes(lng, lat)) +
       geom_raster(aes(fill=surface_o1)) +
       geom_contour(aes(z=surface_o1), col="white", linetype=1, alpha=.5) +
       geom_point(data=spatial_reg, aes(fill=soiliness), colour="white", pch=21, size=3) +
       viridis::scale_fill_viridis(option="plasma", na.value="darkblue") +
       ggtitle("Trend Surface: Order 1") +
       theme_minimal()
</code></pre>

<img src="images/20190530_spatialregression/surface_1.png" alt="surface_1">

<pre><code class="language-r">ggplot(surf_grid, aes(lng, lat)) +
       geom_raster(aes(fill=surface_o2)) +
       geom_contour(aes(z=surface_o2), col="white", linetype=1, alpha=.5) +
       geom_point(data=spatial_reg, aes(fill=soiliness), colour="white", pch=21, size=3) +
       ggtitle("Trend Surface Order 2") +
       viridis::scale_fill_viridis(option="plasma", na.value="darkblue") +
       theme_minimal()
</code></pre>

<img src="images/20190530_spatialregression/surface_2.png" alt="surface_2">

      <p>Can we get a fit that resembles more what the data looks like? The answer
      is yes, as we will see next.</p>

      <a name="covariance"></a>
      <span class="byline">5. Covariance Functions</span>

      <p>What we need from a model to produce a better fit is that it works
      under the assumption that points which are close are likely to have
      similar target values (soiliness in this case). In other words, we
      expect the observed target values to change together across space.<p>

      <p>When want to measure how much two variables change together, we use
      the covariance function. Under the right assumptions, we can also use the
      covariance function to describe the similarity of the observed values
      based on their location.</p>

      <p><b>Covariance function:</b> A function \(K: S \times S → R\) is a
      covariance function if it is continuous, symmetric and positive
      definite.</p>

      <p>For example, assume that \(S \subset R\) and that
      \(r = ||(x_{i1},x_{i2})^\top - (x_{j1},x_{j2})^\top ||\).<p>

      <p>The <i>exponentiated quadratic</i> or <i>RBF</i> covariance function
      is defined as</p>

      <p>\(K(x_i, x_j) = \sigma^2 exp \left( \frac{- r^2 }{ 2 \ell^2} \right)
      \), for some positive parameters \(\sigma\) and
      \(\ell\).</p>


      <p>The <i>rational quadratic</i> covariance function is defined as</p>

      <p>\(K(x_i, x_j) = \sigma^2  \left( 1 + \frac{ r^2 }{ 2 \alpha \ell^2}
      \right) ^{-\alpha}\) for some positive parameters \(\sigma\), \(\alpha\)
      and \(\ell\).</p>

      <p>The <i>power exponential</i> covariance function is defined as</p>

      <p>\(K(x_i, x_j) = \sigma^2   exp\left( - \left(\frac{ r }{ \ell} \right)
      ^{\gamma} \right)\) for some positive parameters \(\sigma\) and
      \(\ell\).</p>

      <p>The election of which covariance function to use depends on our
      assumptions about the change in the association between the points across
      space (eg., the speed of decay).</p>

<pre><code class="language-r">## Code to plot covariance functions
x_0 <- as.matrix(0, nrow=1, ncol=1)
x_linspace <- as.matrix(seq(-8, 8, length.out = 100), ncol=1)
kval <- c(as.numeric(rbf(x_0, x_linspace, sigma=1, lengthscale=1)),
       as.numeric(rqd(x_0, x_linspace, sigma=1, lengthscale=1, alpha=1)),
       as.numeric(pow(x_0, x_linspace, sigma=1, lengthscale=1, gamma=1)))
kname <- c(rep("exp_quad", 100), rep("rat_quad", 100), rep("pow_exp", 100))
covdf <- data.frame(x=rep(x_linspace, 3), k=kval, covariance= kname)

ggplot(covdf, aes(x, k)) + geom_line(aes(col=covariance)) +
       ggtitle("Different Covariance Functions") +
       theme_minimal()
</code></pre>

    <p>Here is an example in 2 dimensions to illustrate the concept of the
    covariance function. Suppose we have 10 locations randomly selected in our
    in our \(400m \times 400m\) plot.</p>

<pre><code class="language-r">## Generate 10 data points
xx <- random_points_ordered(n_points = 10, seed=0)
ggplot(as.data.frame(xx), aes(lng, lat)) +
       geom_point(col="steelblue", size=2.5) +
       theme_minimal()
</code></pre>

<img src="images/20190530_spatialregression/10points.png" alt="10points">

    <p> Based on the pairwise distances between the points, the resulting
    covariance matrix under a Rational Quadratic model is the following.</p>

<pre><code class="language-r">## Spatial covariance between points
K <- rqd(xx, xx, lengthscale=20, alpha=2)
kdf <- data.frame(k = as.vector(K),
                  point_i = factor(rep(10:1, 10)),
                  point_j = factor(sort(rep(1:10, 10), decreasing=TRUE)))

ggplot(kdf, aes(point_j, point_i)) +
       geom_raster(aes(fill=k)) +
       viridis::scale_fill_viridis(option="plasma") +
       ggtitle("Covariance Matrix") +
       scale_y_discrete(limits=rev(levels(kdf$point_i))) +
       theme_minimal()
</code></pre>

<img src="images/20190530_spatialregression/spatial_cov.png" alt="spatial_cov">


      <a name="geostatistics"></a>
      <span class="byline">6. Geostatistical process</span>

      <p>Now that we have discussed how the covariance function can help
      describe properties we observe in our data, we can discuss how to
      incorporate this ideas into a model.</p>

      <p>Before, we mentioned that a surface trend could be modeled as</p>

      <p>\(y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i1}
      x_{i2} + \epsilon_i\).</p>

      <p>Instead we will model our data as</p>

      <p>\(y_i = f_i(x_i) + \epsilon_i\), where \(f = (f_1, \ldots, f_n)^\top\)
      is a multivariate normal distribution.</p>

      <p><i>Why?</i> A Gaussian distribution is completely defined by its first
      two moments. Moreover, a centered Gaussian distribution is uniquely
      defined by its covariance. This means that in our new model, for every
      \(y_i\) we will have now an underlying Gaussian variable \(f_i\) with the
      property that all \({f_i}\) have a spatial dependence structure given by
      \(K(x_i, x_j)\).</p>

      <p><b>Gaussian process:</b> A Gaussian process is a collection of random
      variables, any finite number of which have a joint Gaussian
      distribution.</p>

      <p>We denote a Gaussian process with mean function $m$ and covariance
      function \(K\) as \(f(x) \sim \mathcal{GP}\big(m(x), K(x, x)\big)\).</p>

      <p>While in the linear regression model we estimate the $\beta$
      parameters, here we estimate the values of \(f_i\) at the locations
      \(x_i\). We have to be aware that \(f_i\) are not parameters in the sense
      of classical statistics, they are random variables and we will handle
      them under a Bayesian approach. Given the observed data \(\{x, y\}\), the
      posterior distribution of \(f\) is given by:</p>

      <p>\(p(f|y,x) \propto p(y|f,x) p(f|x)\)</p>

      <p>We will assume for the moment that \(m(x)=0\). This is a common
      assumption that doesn't reduce the flexibility of the method, but
      simplifies handling it. Notice \(\epsilon = (\epsilon_1, \ldots,
      \epsilon_n)^\top\) is joint Gaussian with mean zero and Covariance matrix
      \(I \sigma^2_\epsilon\). Thus \(y \sim N(0, K(x,x) +
      I\sigma^2_\epsilon)\).
      Then it turns out that \(f_\ast|y,x\) (posterior of \(f\) at new locations
      \(x_\ast\)) is multivariate Gaussian with mean</p>

      <p>\(E(f_\ast|y,x) = K(x_\ast,x)(K(x,x) + I\sigma^2_\epsilon)^{-1}y\)</p>

      <p>and covariance</p>

      <p>\(Var(f_\ast|y,x) =K(x_\ast,x_\ast) - K(x_\ast,x)(K +
      I\sigma^2_\epsilon)^{-1}  K(x, x_\ast)\).</p>

      <p>Usually, we want to predict new realizations of \(y_\ast\), instead of
      \(f_\ast\). The predictive values of \(y_\ast\), given the observations
      are computed as</p>

      <p>\(p(y_\ast|y,x) =  \int p(y_\ast  |f_\ast) p(f_\ast|y,x)
      df_\ast\).</p>

      <p>In this case, since $y$ is just a noisy version of $f$ where the noise
      is Gaussian with zero mean, it turns out that</p>

      <p>\(E(y_\ast|y,x) = K(x_\ast,x)(K(x,x) + I\sigma^2_\epsilon)^{-1}y\)</p>

      <p>and</p>

      <p>\(Var(y_\ast|y,x) =K(x_\ast,x_\ast) - K(x_\ast,x)(K +
      I\sigma^2_\epsilon)^{-1}  K(x, x_\ast) + I\sigma^2_\epsilon\).</p>

      <p>Going back to our soil example, this is how a Gaussian process
      regression fit looks like.</p>

<pre><code class="language-r"># For the moment we will assume that the following quantities are known
rbf_sigma <- 20
rbf_lengthscale <- 25
epsilon_sigma <- 10

# These are the locations of the train set and predictions
X <- as.matrix(spatial_reg[, c("lng", "lat")])
X_star <- as.matrix(surf_grid[, c("lng", "lat")])

# Output value
Y <- as.matrix(spatial_reg$soiliness)

# Fit a Gaussian process
gp_fit <- gp_regression_rbf(X, Y, X_star, k_var=rbf_sigma^2, k_len=rbf_lengthscale, noise_var=epsilon_sigma^2)
surf_grid$surface_gp_mean <- gp_fit$gp_mean
surf_grid$surface_gp_var <- gp_fit$gp_var

ggplot(surf_grid, aes(lng, lat)) + 
       geom_raster(aes(fill=surface_gp_mean)) +
       geom_contour(aes(z=surface_gp_mean), col="white", linetype=1, alpha=.5) +
       viridis::scale_fill_viridis(option="plasma", na.value="darkblue") +
       ggtitle("Gaussian Process - Predictive Mean") +
       theme_minimal()
</code></pre>

<img src="images/20190530_spatialregression/gp_mean.png" alt="gp_mean">

<pre><code class="language-r">ggplot(surf_grid, aes(lng, lat)) +
     geom_raster(aes(fill=surface_gp_var)) +
     geom_contour(aes(z=surface_gp_var), col="white", linetype=1, alpha=.5) +
     geom_point(data=spatial_reg, colour="white", pch=21, size=3) +
     viridis::scale_fill_viridis(option="plasma", na.value="darkblue") +
     ggtitle("Gaussian Process - Predictive Variance") +
     theme_minimal()
</code></pre>

<img src="images/20190530_spatialregression/gp_var.png" alt="gp_var">

      <p>Notice that the regions with no points have higher variance, while the
      regions with many points concentrated have a low variance. Remember that
      in the Bayesian approach, the variance is a measure of uncertainty. Thus,
      we are more uncertain about the process when there are no observations
      nearby and become more certain the more observations we have.</p>

      <p>Since the Gaussian process is defined by its mean and covariance,
      these are usually the parameters we are mostly interested in estimating.
      However these are not the only parameters of the model. As we discussed
      before, the covariance functions have different parameters associated,
      for example \(\sigma\), \(\ell\) or \(\gamma\). We usually refer to them as
      <i>hyperparameters</i>.</p>

      <p>The values of the hyperparameters can also be chosen based on the
      data. While the optimal solutions for the posterior mean and variance
      have a nice and simple analytical expression, as we saw above, estimating
      optimal values for the hyperparamters is not that straightforward.</p>

      <p>When taking into account the hyperparameters the posterior
      distribution of \(f\) should be written as</p>

      <p>\(p(f|y,x,\theta) \propto p(y|f,\theta) p(f|x, \theta) \).</p>

      <p>The normalizing constant of the expression above is given by</p>

      <p>\(p(y|x,\theta) = \int p(y|f,\theta) p(f|x, \theta) df\),</p>

      <p>which can be used to calculate the posterior distribution of
      \(\theta\)</p>

      <p>\(p(\theta|y,x) \propto p(y|x,\theta) p(\theta)\),</p>

      <p>with normalizing constant</p>

      <p>\(p(y|x) = \int p(y|x,\theta) p(\theta) d\theta\).</p>

      <p>Solving the integral above can be a hard task, and we can rely on
      Markov Chain Monte Carlo methods to do so.
      An alternative is to find the value of \(\theta\) that maximizes the <i>marginal likelihood</i>
      \(p(y|x,\theta)\). Here the term marginal refers to the fact that
      \(p(y|x,\theta)\) is computed after marginalizing the latent function
      \(f\). By following this approach we are choosing the combination of \(\theta\)
      values where the likelihood of the data observed \(y\) is the highest. This
      approach is know as Maximum Likelihood type II.</p>

      <p>Below is an example of how to use Monte Carlo methods to estimate the
      posterior distribution of the hyperparameters using
      <a href="http://mc-stan.org/users/interfaces/rstan"><i>RStan</i></a>.</p>

<pre><code class="language-r">fit_hyper <- rstan::stan(file="stan_code/gp_gaussian_fit_hyper.stan",
         data=list(n_data = nrow(spatial_reg),
                                 n_dim = 2,
                                 y_data = spatial_reg$soiliness,
                                 X_data = spatial_reg[, c("lng", "lat")],
                                 mu_data = rep(0, nrow(spatial_reg))),
         pars = c("noise_var", "cov_var", "cov_length"),
         warmup = 1000, iter = 5000, chains = 4,
         verbose = FALSE, seed=123)

hyper_params <- rstan::extract(fit_hyper)
noise_var <- mean(cbind(hyper_params$noise_var))
cov_var <- mean(cbind(hyper_params$cov_var))
cov_length <- colMeans(cbind(hyper_params$cov_length))
</code></pre>

      <p>We will use the mean values of the samples generated as point
      estimates and show the predicted surface of the Gaussian process.</p>

      <p><b>Note:</b> Our stan implementation considers a different lengthscale
      for each dimension in X, while our RBF function considers a single
      parameter. For the moment we will use as lengthscale value the average of
      both lenghtscale parameters.</p>

<pre><code class="language-r"># Fit a Gaussian process
gp_fit_stan <- gp_regression_rbf(X, Y, X_star, k_var=cov_var, k_len=mean(cov_length), noise_var=noise_var)
surf_grid$surface_gp_mean_stan <- gp_fit_stan$gp_mean
surf_grid$surface_gp_var_stan <- gp_fit_stan$gp_var

# Plot predictive mean
ggplot(surf_grid, aes(lng, lat)) +
       geom_raster(aes(fill=surface_gp_mean_stan)) +
       geom_contour(aes(z=surface_gp_mean_stan), col="white", linetype=1, alpha=.5) +
       geom_point(data=spatial_reg, aes(fill=soiliness), colour="white", pch=21, size=3) +
       viridis::scale_fill_viridis(option="plasma", na.value="darkblue") +
       ggtitle("Gaussian Process - Predictive Mean") +
       theme_minimal()
</code></pre>

<img src="images/20190530_spatialregression/gp_stanfit.png" alt="gp_stanfit">

      <p>So far all the discussion has been around spatial location. _What about
      additional information we may have?_. We can have different approaches to
      include it.</p>

      <p>For example, we can define a model as follows</p>

      <p>\(y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_q x_{iq} + f(x_{is})
      + \epsilon_i\), where</p>

      <p>\(\{x_{ij}\}_{j=1}^{q}\) are different covariantes and \(x_{is}\) are
      spatial coordinates.</p>

      <p>Also \(f(x_{is})\) does not have to be constrained to a 2-dimensional
      space. We can use \(f(x^\ast_i)\), where \(x^\ast_i\) is a vector that
      includes the spatial locations and other data features. In such a case,
      these features will also have a non-linear effect. For example, if the
      input vector contains spatial location and a temporal reference, \(x^\ast
      = (x^{(s)}, x^{(t)})\), a covariance function for a space-time process
      can be defined as:</p>

      <p>\(K(x^\ast_i, x^\ast_j) = \sigma^2 exp \left( -\frac{||x^{(s)}_i -
      x^{(s)}_j||^2 }{ 2  \ell^2_s} - \frac{||x^{(t)}_i - x^{(t)}_j||^2 }{ 2
      \ell^2_t} \right)\).</p>

      <p>Note that the expression above, is in fact the product of two
      covariance functions:</p>

      <p>\(K(x^\ast_i, x^\ast_j) = \sigma^2 exp \left( -\frac{||x^{(s)}_i -
      x^{(s)}_j||^2 }{ 2  \ell^2_s} \right) exp \left(- \frac{||x^{(t)}_i -
      x^{(t)}_j||^2 }{ 2  \ell^2_t} \right)\).</p>

      <p>It is also possible to use sums of covariance functions to have
      additive effects of the form</p>

      <p>\(y_i = f_1(x^\ast_i) + f_2(x^\ast_i) + \epsilon_i\).</p>

      <a name="references"></a>
      <span class="byline">7. References</span>

      <ul>
        <li>- Cressie, N. and Wikle, C. K. (2011). Statistics for spatio-temporal
          data. John Wiley & Sons.</li>

        <li>- Diggle, P. J., Moraga, P., Rowlingson, B., Taylor, B. M., et al.
          (2013). Spatial and spatio-temporal log-Gaussian Cox processes:
          Extending the geostatistical paradigm. Statistical Science,
          28(4):542–563.</li>

        <li>- Diggle, P. J., Tawn, J., and Moyeed, R. (1998). Model-based
          geostatistics. Journal of the Royal Statistical Society: Series C
          (Applied Statistics), 47(3):299–350.</li>

        <li>- Fisher, R. A. (1935). The design of experiments. Oliver &
          Boyd.</li>

        <li>- Gandin, L. S. (1963). Ob“ektivnyi analiz meteorologicheskikh
          polei. Gidrometeologicheskoe Izdatel’stvo, Leningrad. Translation
          (1965): Objective analysis of meteorological fields. Israel Program
          for Scientific Translations, Jerusalem.</li>

        <li>- Matheron, G. (1962). Traité de géostatistique appliquée, tome
          I. Mémoires du Bureau de Recherche Géologiques et Minières, No.
          14. Editions Technip, Paris.</li>

        <li>- Matheron, G. (1963). Traité de géostatistique appliquée, tome
          II: le krigeage. Mémoires du Bureau de Recherche Géologiques et
          Minières, No. 24. Editions Bureau de Recherche Géologiques et
          Minières, Paris.</li>

        <li>- Rasmussen, Carl Edward. "Gaussian processes in machine
          learning." Advanced lectures on machine learning. Springer,
          Berlin, Heidelberg, 2004. 63-71.</li>
      </ul>

    </article>
  </div>
</div>

    <!-- JAVASCRIPTS -->
    <a id="backtotop" href="#top"><i class="fa fa-chevron-up"></i></a>
    <script src="basend_layout/scripts/jquery.backtotop.js"></script>
    <!-- <div id="content" class="mobileUI-main-content">
  <div id="content-inner">
    <div class="pager">
      <!--
      <a href="#" class="button previous">Previous Page</a>
      <div class="pages">
        <a href="#" class="active">1</a> <a href="#">2</a> <a href="#">3</a> <a href="#">4</a> <span>&hellip;</span> <a href="#">20</a>
      </div>
      <a href="#" class="button next">Next Page</a>
      -->
    </div>
  </div>
</div>
 -->

  </div>
</body>
</html>
